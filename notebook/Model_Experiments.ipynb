{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JupyterHub Notebook\n",
    "\n",
    "### This notebook server is hosted on the OpenShift platform which provides a separate server for each individual user. The platform takes care of the provisioning of the server and allocating related to storage.\n",
    "\n",
    "### This notebook server is hosted on the OpenShift platform which provides a separate server for each individual user. The platform takes care of the provisioning of the server and allocating related to storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install seaborn\n",
    "%pip install matplotlib\n",
    "%pip install six\n",
    "%pip install sklearn\n",
    "%pip install yellowbrick\n",
    "%pip install pydotplus\n",
    "%pip install graphviz\n",
    "%pip install verta\n",
    "%pip install Minio\n",
    "%pip install category_encoders\n",
    "# %pip install modin[ray]\n",
    "%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"ray\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# import modin.pandas as pd\n",
    "\n",
    "import watermark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datetime import datetime\n",
    "import verta.integrations.sklearn\n",
    "from minio import Minio\n",
    "from verta import Client\n",
    "from minio.error import ResponseError\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# import tools as tools\n",
    "%matplotlib inline\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%watermark -n -v -m -g -iv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this next section, on the third line, change experiment_name by appending your username to _customerchurn_, e.g., if your username is user1: \n",
    "#### _experiment_name = \"customerchurn\"+\"user1\"_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dateTimeObj = datetime.now()\n",
    "timestampStr = dateTimeObj.strftime(\"%d%Y%H%M%S%f\")\n",
    "experiment_name = \"customerchurn\"\n",
    "experiment_id = experiment_name + timestampStr\n",
    "\n",
    "def get_s3_server():\n",
    "    minioClient = Minio('minio-ml-workshop:9000',\n",
    "                    access_key='minio',\n",
    "                    secret_key='minio123',\n",
    "                    secure=False)\n",
    "\n",
    "    return minioClient\n",
    "\n",
    "def get_verta():\n",
    "    client = Client(\"http://v1-webapp:3000\")\n",
    "    return client\n",
    "\n",
    "def get_meta_store():\n",
    "    client = get_verta()\n",
    "    proj = client.set_project(\"ml-workshop\")\n",
    "    client.set_experiment(experiment_name)\n",
    "    run = client.set_experiment_run(experiment_id)\n",
    "    return run\n",
    "\n",
    "\n",
    "# def record_metrics(classifier, expereiment_name, accuracy_score, hyperparameters):\n",
    "#     client = Client(\"http://chart-1603715522-webapp:3000\")\n",
    "#     proj = client.set_project(\"HDFC DEmo PRoject\")\n",
    "#     client.set_experiment(\"Iris Classifier\")\n",
    "#     run = client.set_experiment_run(expereiment_name)\n",
    "#\n",
    "#     for key, value in hyperparameters.items():\n",
    "#         run.log_hyperparameters({key : value})\n",
    "#\n",
    "#     run.log_metric('accuracy', accuracy_score)\n",
    "#     run.log_tags([classifier])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull in data from Minio object storage (data that was cleansed previously in notebook _Merge_Data.ipynb_). Replace _[INSERT_FILENAME]_ on line 2 with your filename in Minio's data folder. Use pandas _read_csv_ function to read the data into a DataFrame, or 2 dimensional data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "minioClient = get_s3_server()\n",
    "data_file = minioClient.fget_object(\"data\", \"full_data_csv/data.csv/part-00000-40d2e216-2992-4781-ba15-9789cb64ae30-c000.csv\", \"/tmp/data.csv\")\n",
    "data_file_version = data_file.version_id\n",
    "data = pd.read_csv('/tmp/data.csv')\n",
    "data.head(5)\n",
    "\n",
    "# from verta.dataset import S3\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n",
    "#\n",
    "# dataset = get_verta().get_dataset(name=\"MLWorkshop Churn Data\")\n",
    "# dataset_version = dataset.create_version(S3(\"s3://data/full_Data_csv/part-00000-56275f21-afd6-4049-a32a-e0252789a607-c000.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install boto3\n",
    "# import boto3\n",
    "# from verta.dataset import S3\n",
    "# from verta.dataset import Path\n",
    "# from boto3.client import Config\n",
    "#\n",
    "# # os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n",
    "# # os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n",
    "# # os.environ[\"endpoint_url\"] = \"minio-ml-workshop:9000\"\n",
    "#\n",
    "# # s3 = boto3.resource('s3',\n",
    "# #   endpoint_url='http://minio-ml-workshop:9000',\n",
    "# #   config=boto3.session.Config(signature_version='s3v4')\n",
    "# # )\n",
    "#\n",
    "# s3 = boto3.resource('s3',\n",
    "#                     endpoint_url='http://minio-ml-workshop:9000',\n",
    "#                     aws_access_key_id='minio',\n",
    "#                     aws_secret_access_key='minio123',\n",
    "#                     config=Config(signature_version='s3v4'),\n",
    "#                     region_name='us-east-1')\n",
    "# #\n",
    "# dataset= get_verta('123').get_dataset(name=\"MLWorkshop Churn Data\")\n",
    "#\n",
    "# dataset_version = dataset.create_version(  S3(\"s3://data/full_Data_csv/part-00000-56275f21-afd6-4049-a32a-e0252789a607-c000.csv\"))\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pandas.DataFrame functions\n",
    "- _shape_ to return the dimensionality\n",
    "- _info_ to print a concise summary of the DataFrame\n",
    "- _describe_ to generate descriptive statistics of the DataFrame's columns\n",
    "- _isnull().sum()_ to sum the empty values\n",
    "- finally determine Churn and Total Changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert binary variable into numeric so plotting is easier. We need to later take mean\n",
    "data['Churn'] = data['Churn'].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.replace(\" \", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mean = data['TotalCharges'].mean()\n",
    "data.fillna(mean, inplace=True)\n",
    "# Now we know that total charges has nan values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering pipeline\n",
    "### Use category_encoder's Ordinal encoding method which uses a single column of integers to represent the classes - then fit that to our 2 dimensional data imported earlier. Then pickle it and transform it. Then use Onehot (or dummy) coding for categorical features, producing one feature per category, each binary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "import joblib\n",
    "\n",
    "names = ['gender', 'Partner', 'Dependents', 'PhoneService', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'Churn']\n",
    "# for column in names:\n",
    "#     labelencoder(column)\n",
    "\n",
    "enc = ce.ordinal.OrdinalEncoder(cols=names)\n",
    "enc.fit(data)\n",
    "joblib.dump(enc, 'enc.pkl')\n",
    "labelled_set = enc.transform(data)\n",
    "labelled_set.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "names = ['MultipleLines', 'InternetService', 'Contract', 'PaymentMethod', 'OnlineSecurity', 'OnlineBackup',\n",
    "         'DeviceProtection', 'TechSupport']\n",
    "\n",
    "ohe = ce.OneHotEncoder(cols=names)\n",
    "ohe.fit(labelled_set)\n",
    "joblib.dump(ohe, 'ohe.pkl')\n",
    "final_set = ohe.transform(labelled_set)\n",
    "final_set.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Now we use scikit-learn's 'train_test_split' function to randomly split our data into training and testing sets. Then remove the _Churn_ and _customerID_ fields from our training and testing datasets and output the shaope of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels = final_set['Churn']\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_set, labels, test_size=0.2)\n",
    "X_train.pop('Churn')\n",
    "X_train.pop('customerID')\n",
    "X_test.pop('Churn')\n",
    "X_test.pop('customerID')\n",
    "print ('Training Data Shape',X_train.shape, y_train.shape)\n",
    "print ('Testing Data Shape',X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output data For cross validation and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Data For cross validation and GridSearch\n",
    "Y = final_set['Churn']\n",
    "X = final_set.drop(['Churn', 'customerID'], axis=1)\n",
    "print ('Training Data Shape', X.shape)\n",
    "print ('Testing Data Shape', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DecisionTreeClassifier object, extract hyper parameters, and then the best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create decision tree object\n",
    "DT = DecisionTreeClassifier()\n",
    "# List of parameters\n",
    "# entropy\n",
    "criterion = ['gini']\n",
    "max_depth = [5,10,15]\n",
    "min_samples_split = [2,4,6]\n",
    "min_samples_leaf = [4,5,6,8]\n",
    "# Save all the lists in the variable\n",
    "hyperparameters = dict(max_depth=max_depth, criterion=criterion,min_samples_leaf = min_samples_leaf ,min_samples_split = min_samples_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = GridSearchCV(DT, hyperparameters, cv=5, verbose=0)\n",
    "best_model = model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract best scores, params, creteria and depth from our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mean cross validated score\n",
    "print('Mean Cross-Validated Score: ',best_model.best_score_)\n",
    "print('Best Parameters',best_model.best_params_)\n",
    "# You can also print the best penalty and C value individually from best_model.best_estimator_.get_params()\n",
    "print('Best criteria:', best_model.best_estimator_.get_params()['criterion'])\n",
    "print('Best depth:', best_model.best_estimator_.get_params()['max_depth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use K-Folds cross-validator to split data in train/test sets. Create a dictionary of hyperparameter candidates, train model using a DecisionTreeClassifier, assess results, print and store hyper parameters and accuracy and tag using 'DecisionTreeClassifier'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits = 3)\n",
    "hyperparameters = dict(max_depth=5, criterion='gini',min_samples_leaf = 3 ,min_samples_split = 10)\n",
    "model = DecisionTreeClassifier(max_depth=5, criterion='gini',min_samples_leaf = 3 ,min_samples_split = 10)\n",
    "model = model.fit(X_train, y_train)\n",
    "joblib.dump(model, 'dct.pkl')\n",
    "results = model_selection.cross_val_score(model,X,Y,cv = kfold)\n",
    "print(results)\n",
    "print('Accuracy',results.mean()*100)\n",
    "store = get_meta_store()\n",
    "store.log_hyperparameters(hyperparameters)\n",
    "store.log_model(model)\n",
    "store.log_metric('Accuracy',results.mean()*100)\n",
    "store.log_tag(\"DecisionTreeClassifier\")\n",
    "# get_meta_store().log_dataset_version(\"raw_data\", dataset_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like before, in this next section, on the third line, change experiment_name by appending your username to _customerchurn_, e.g., if your username is user1: \n",
    "#### _experiment_name = \"customerchurn\"+\"user1\"_\n",
    "### Create RandomForestClassifier object, extract hyper parameters, and then the best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dateTimeObj = datetime.now()\n",
    "timestampStr = dateTimeObj.strftime(\"%d%Y%H%M%S%f\")\n",
    "experiment_name = \"customerchurn\"\n",
    "experiment_id = experiment_name + timestampStr\n",
    "\n",
    "\n",
    "# Create random forest object\n",
    "RF = RandomForestClassifier()\n",
    "n_estimators = [18,22]\n",
    "criterion = ['gini', 'entropy']\n",
    "# Create a list of all of the parameters\n",
    "max_depth = [30,40,50]\n",
    "min_samples_split = [6,8]\n",
    "min_samples_leaf = [8,10,12]\n",
    "# Merge the list into the variable\n",
    "hyperparameters = dict(n_estimators = n_estimators,max_depth=max_depth, criterion=criterion,min_samples_leaf = min_samples_leaf ,min_samples_split = min_samples_split)\n",
    "# Fit your model using gridsearch\n",
    "model = GridSearchCV(RF, hyperparameters, cv=5, verbose=0)\n",
    "best_model = model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract best scores, params, creteria and depth from our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mean cross validated score\n",
    "print('Mean Cross-Validated Score: ',best_model.best_score_)\n",
    "print('Best Parameters',best_model.best_params_)\n",
    "# You can also print the best penalty and C value individually from best_model.best_estimator_.get_params()\n",
    "print('Best criteria:', best_model.best_estimator_.get_params()['criterion'])\n",
    "print('Best depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best estimator:', best_model.best_estimator_.get_params()['n_estimators'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As above, use K-Folds cross-validator to split data in train/test sets. Create a dictionary of hyperparameter candidates, train model using a RandomForestClassifier, assess results, print and store hyper parameters and accuracy and tag using 'RandomForestClassifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits = 3)\n",
    "hyperparameters = dict(max_depth=40, criterion='gini',min_samples_leaf = 12 ,min_samples_split = 8, n_estimators = 22)\n",
    "model = RandomForestClassifier(max_depth=40, criterion='gini',min_samples_leaf = 12 ,min_samples_split = 8, n_estimators = 22)\n",
    "model = model.fit(X_train, y_train)\n",
    "joblib.dump(model, 'rft.pkl')\n",
    "results = model_selection.cross_val_score(model,X,Y,cv = kfold)\n",
    "print(results)\n",
    "print('Accuracy',results.mean()*100)\n",
    "store = get_meta_store()\n",
    "store.log_hyperparameters(hyperparameters)\n",
    "store.log_model(model)\n",
    "store.log_metric('Accuracy',results.mean()*100)\n",
    "store.log_tag(\"RandomForestClassifier\")\n",
    "store.log_attribute(\"data_file_location\", \"data/full_data_csv/a.csv\")\n",
    "store.log_attribute(\"data_file_version\", data_file_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
